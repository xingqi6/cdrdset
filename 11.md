#!/usr/bin/env python3
"""
Hugging Face Datasets 虚拟存储驱动
将 Cloudreve 文件存储到 Datasets 仓库，突破 50GB 限制
"""

import os
import subprocess
import json
import hashlib
import time
from pathlib import Path

class DatasetsStorage:
    """
    Hugging Face Datasets 存储驱动
    原理：
    1. 将大文件分块（50MB/块）
    2. 使用 Git LFS 上传到 Datasets 仓库
    3. 本地只保存元数据，不保存文件内容
    """
    
    def __init__(self):
        self.hf_token = os.environ.get("HF_TOKEN", "")
        self.dataset_repo = os.environ.get("HF_DATASET_REPO", "")  # 格式: username/dataset-name
        self.chunk_size = 50 * 1024 * 1024  # 50MB 分块
        
        # 本地缓存目录（临时）
        self.cache_dir = "/tmp/datasets_cache"
        self.metadata_dir = "/usr/local/sys_kernel/datasets_meta"
        
        os.makedirs(self.cache_dir, exist_ok=True)
        os.makedirs(self.metadata_dir, exist_ok=True)
        
        # 初始化 Datasets 仓库
        self.repo_path = f"{self.cache_dir}/repo"
        self._init_repo()
    
    def _init_repo(self):
        """初始化或克隆 Datasets 仓库"""
        if not self.hf_token or not self.dataset_repo:
            print(">>> [Storage] WARNING: HF_TOKEN or HF_DATASET_REPO not set!")
            return False
        
        if os.path.exists(self.repo_path):
            print(f">>> [Storage] Repo exists: {self.repo_path}")
            return True
        
        try:
            # 克隆仓库
            repo_url = f"https://huggingface.co/datasets/{self.dataset_repo}"
            cmd = [
                "git", "clone",
                f"https://oauth2:{self.hf_token}@huggingface.co/datasets/{self.dataset_repo}",
                self.repo_path
            ]
            subprocess.run(cmd, check=True, capture_output=True)
            
            # 配置 Git LFS
            subprocess.run(["git", "lfs", "install"], cwd=self.repo_path, check=True)
            
            # 配置 Git
            subprocess.run(["git", "config", "user.email", "cloudreve@hf.space"], 
                          cwd=self.repo_path, check=True)
            subprocess.run(["git", "config", "user.name", "Cloudreve Storage"], 
                          cwd=self.repo_path, check=True)
            
            print(f">>> [Storage] Initialized repo: {self.dataset_repo}")
            return True
            
        except Exception as e:
            print(f">>> [Storage] Failed to init repo: {e}")
            return False
    
    def upload_file(self, local_path, remote_name):
        """
        上传文件到 Datasets
        Args:
            local_path: 本地文件路径
            remote_name: 远程文件名（包含路径）
        Returns:
            dict: 文件元数据
        """
        if not os.path.exists(local_path):
            raise FileNotFoundError(f"File not found: {local_path}")
        
        file_size = os.path.getsize(local_path)
        file_hash = self._calculate_hash(local_path)
        
        # 生成元数据
        metadata = {
            "name": remote_name,
            "size": file_size,
            "hash": file_hash,
            "chunks": [],
            "upload_time": time.time()
        }
        
        # 分块上传
        chunk_index = 0
        with open(local_path, 'rb') as f:
            while True:
                chunk_data = f.read(self.chunk_size)
                if not chunk_data:
                    break
                
                chunk_name = f"{file_hash}_{chunk_index}.chunk"
                chunk_path = os.path.join(self.repo_path, "chunks", chunk_name)
                
                # 创建目录
                os.makedirs(os.path.dirname(chunk_path), exist_ok=True)
                
                # 写入分块
                with open(chunk_path, 'wb') as cf:
                    cf.write(chunk_data)
                
                metadata["chunks"].append({
                    "index": chunk_index,
                    "name": chunk_name,
                    "size": len(chunk_data)
                })
                
                chunk_index += 1
                print(f">>> [Storage] Uploaded chunk {chunk_index}/{(file_size + self.chunk_size - 1) // self.chunk_size}")
        
        # 保存元数据
        self._save_metadata(remote_name, metadata)
        
        # 提交到 Git
        self._commit_and_push(f"Upload: {remote_name}")
        
        # 删除本地临时文件
        try:
            os.remove(local_path)
        except:
            pass
        
        return metadata
    
    def download_file(self, remote_name, local_path):
        """
        从 Datasets 下载文件
        Args:
            remote_name: 远程文件名
            local_path: 本地保存路径
        """
        metadata = self._load_metadata(remote_name)
        if not metadata:
            raise FileNotFoundError(f"File not found in storage: {remote_name}")
        
        # 确保仓库是最新的
        self._pull_repo()
        
        # 合并分块
        with open(local_path, 'wb') as f:
            for chunk_info in metadata["chunks"]:
                chunk_path = os.path.join(self.repo_path, "chunks", chunk_info["name"])
                
                if not os.path.exists(chunk_path):
                    raise FileNotFoundError(f"Chunk not found: {chunk_info['name']}")
                
                with open(chunk_path, 'rb') as cf:
                    f.write(cf.read())
                
                print(f">>> [Storage] Downloaded chunk {chunk_info['index'] + 1}/{len(metadata['chunks'])}")
        
        return metadata
    
    def delete_file(self, remote_name):
        """删除文件"""
        metadata = self._load_metadata(remote_name)
        if not metadata:
            return False
        
        # 删除所有分块
        for chunk_info in metadata["chunks"]:
            chunk_path = os.path.join(self.repo_path, "chunks", chunk_info["name"])
            try:
                os.remove(chunk_path)
            except:
                pass
        
        # 删除元数据
        metadata_path = os.path.join(self.metadata_dir, f"{self._safe_filename(remote_name)}.json")
        try:
            os.remove(metadata_path)
        except:
            pass
        
        # 提交删除
        self._commit_and_push(f"Delete: {remote_name}")
        
        return True
    
    def list_files(self):
        """列出所有文件"""
        files = []
        for filename in os.listdir(self.metadata_dir):
            if filename.endswith('.json'):
                metadata = self._load_metadata(filename[:-5])
                if metadata:
                    files.append({
                        "name": metadata["name"],
                        "size": metadata["size"],
                        "upload_time": metadata["upload_time"]
                    })
        return files
    
    def _calculate_hash(self, filepath):
        """计算文件 SHA256"""
        sha256 = hashlib.sha256()
        with open(filepath, 'rb') as f:
            while chunk := f.read(8192):
                sha256.update(chunk)
        return sha256.hexdigest()
    
    def _save_metadata(self, remote_name, metadata):
        """保存元数据"""
        metadata_path = os.path.join(self.metadata_dir, f"{self._safe_filename(remote_name)}.json")
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
    
    def _load_metadata(self, remote_name):
        """加载元数据"""
        metadata_path = os.path.join(self.metadata_dir, f"{self._safe_filename(remote_name)}.json")
        if not os.path.exists(metadata_path):
            return None
        with open(metadata_path, 'r') as f:
            return json.load(f)
    
    def _safe_filename(self, name):
        """转换为安全的文件名"""
        return hashlib.md5(name.encode()).hexdigest()
    
    def _commit_and_push(self, message):
        """提交并推送到 Datasets"""
        try:
            subprocess.run(["git", "add", "."], cwd=self.repo_path, check=True)
            subprocess.run(["git", "commit", "-m", message], cwd=self.repo_path)
            subprocess.run(["git", "push"], cwd=self.repo_path, check=True)
            print(f">>> [Storage] Pushed: {message}")
        except Exception as e:
            print(f">>> [Storage] Push failed: {e}")
    
    def _pull_repo(self):
        """拉取最新更新"""
        try:
            subprocess.run(["git", "pull"], cwd=self.repo_path, check=True)
        except:
            pass


# ===========================
# WebDAV 服务器（桥接层）
# ===========================

from http.server import HTTPServer, BaseHTTPRequestHandler
import urllib.parse

class WebDAVHandler(BaseHTTPRequestHandler):
    """
    WebDAV 服务器，将 Cloudreve 的文件操作转换为 Datasets 操作
    """
    
    storage = None  # 将在启动时设置
    
    def do_PUT(self):
        """处理文件上传"""
        try:
            # 解析路径
            path = urllib.parse.unquote(self.path).lstrip('/')
            
            # 获取文件大小
            content_length = int(self.headers.get('Content-Length', 0))
            
            # 保存到临时文件
            temp_path = f"/tmp/upload_{int(time.time())}.tmp"
            with open(temp_path, 'wb') as f:
                f.write(self.rfile.read(content_length))
            
            # 上传到 Datasets
            print(f">>> [WebDAV] Uploading {path} ({content_length} bytes)")
            self.storage.upload_file(temp_path, path)
            
            self.send_response(201)
            self.end_headers()
            
        except Exception as e:
            print(f">>> [WebDAV] Upload failed: {e}")
            self.send_response(500)
            self.end_headers()
    
    def do_GET(self):
        """处理文件下载"""
        try:
            path = urllib.parse.unquote(self.path).lstrip('/')
            
            # 下载到临时文件
            temp_path = f"/tmp/download_{int(time.time())}.tmp"
            metadata = self.storage.download_file(path, temp_path)
            
            # 发送文件
            self.send_response(200)
            self.send_header('Content-Length', str(metadata['size']))
            self.send_header('Content-Type', 'application/octet-stream')
            self.end_headers()
            
            with open(temp_path, 'rb') as f:
                self.wfile.write(f.read())
            
            # 删除临时文件
            os.remove(temp_path)
            
        except FileNotFoundError:
            self.send_response(404)
            self.end_headers()
        except Exception as e:
            print(f">>> [WebDAV] Download failed: {e}")
            self.send_response(500)
            self.end_headers()
    
    def do_DELETE(self):
        """处理文件删除"""
        try:
            path = urllib.parse.unquote(self.path).lstrip('/')
            self.storage.delete_file(path)
            
            self.send_response(204)
            self.end_headers()
            
        except Exception as e:
            print(f">>> [WebDAV] Delete failed: {e}")
            self.send_response(500)
            self.end_headers()
    
    def do_PROPFIND(self):
        """处理文件列表"""
        try:
            files = self.storage.list_files()
            
            # 生成 WebDAV XML 响应
            xml = '<?xml version="1.0" encoding="utf-8"?>\n'
            xml += '<D:multistatus xmlns:D="DAV:">\n'
            
            for file_info in files:
                xml += f'''
  <D:response>
    <D:href>/{file_info["name"]}</D:href>
    <D:propstat>
      <D:prop>
        <D:getcontentlength>{file_info["size"]}</D:getcontentlength>
      </D:prop>
    </D:propstat>
  </D:response>
'''
            
            xml += '</D:multistatus>'
            
            self.send_response(207)
            self.send_header('Content-Type', 'application/xml')
            self.send_header('Content-Length', str(len(xml)))
            self.end_headers()
            self.wfile.write(xml.encode())
            
        except Exception as e:
            print(f">>> [WebDAV] PROPFIND failed: {e}")
            self.send_response(500)
            self.end_headers()


def start_webdav_server(port=8888):
    """启动 WebDAV 服务器"""
    storage = DatasetsStorage()
    WebDAVHandler.storage = storage
    
    server = HTTPServer(('127.0.0.1', port), WebDAVHandler)
    print(f">>> [WebDAV] Server started on port {port}")
    server.serve_forever()


if __name__ == "__main__":
    # 测试
    storage = DatasetsStorage()
    print(f"Storage initialized: {storage.dataset_repo}")
